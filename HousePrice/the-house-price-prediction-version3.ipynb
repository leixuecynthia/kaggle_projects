{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas import Series,DataFrame\n%matplotlib inline\nfrom scipy.stats import norm, skew\nfrom scipy import stats\nimport scipy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df['SalePrice'].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#DROP ID COLUMN\ntrain_ID = train_df['Id']\ntest_ID = test_df['Id']\ntrain_df.drop(\"Id\", axis=1, inplace=True)\ntest_df.drop(\"Id\", axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Histogram of sale prices\nplt.hist(train_df.SalePrice,bins=50)\nplt.xlabel('Price')\nplt.ylabel('Number of the houses')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the LotFrontage\n#plt.scatter(train_df.iloc[:,3],\"SalePrice\")\n\n#plt.ylabel('Sale Price')\n#plt.xlabel('LotFrontage')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"obj=train_df.isnull().sum()\nfor key,value in obj.iteritems():\n    print(key,\",\",value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Numerical feature correlation\nplt.figure(figsize = (20,10))\ninternal_chars = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF',\n                 'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','SalePrice']\ncorrmat = train_df[internal_chars].corr()\nsns.heatmap(corrmat,square=False,linewidths=.5,annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(corrmat[\"SalePrice\"].sort_values(ascending=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"looks like GrLivArea, GarageCars, GarageArea, TotalBsmtSF, 1stFlrSF, FullBath features have high correlation to the saleprice.","metadata":{}},{"cell_type":"markdown","source":"****1.Data Processing****","metadata":{}},{"cell_type":"markdown","source":"1.1 Outliers","metadata":{}},{"cell_type":"code","source":"#Deal with outliers in GrLivArea\nsns.scatterplot(x='GrLivArea',y='SalePrice',data=train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<400000)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<400000)].index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind_drop = train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<400000)].index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.drop(ind_drop,axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x='GrLivArea',y='SalePrice',data=train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1.2 Target Variable","metadata":{}},{"cell_type":"code","source":"sns.displot(train_df['SalePrice'])\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get the probability plot\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","metadata":{}},{"cell_type":"markdown","source":"1.3 Log-transformation of the target","metadata":{}},{"cell_type":"code","source":"train_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n\n#check the new distribution\nsns.distplot(train_df['SalePrice'])\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the probability plot\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The skew seems now corrected and the data appears more normally distributed.\n\n","metadata":{}},{"cell_type":"markdown","source":"**2.Feature engineering**","metadata":{}},{"cell_type":"code","source":"#Concatenate the train and test dataset\ny_train = train_df.SalePrice.values\nall_df = pd.concat((train_df,test_df)).reset_index(drop=True)\nall_df.drop([\"SalePrice\"],axis=1,inplace=True)\n#print(\"all_df size: {}\".format(all_df.shape))\nall_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.1 Missing values","metadata":{}},{"cell_type":"code","source":"#Check the missing value ratio of each feature\nall_df_na = (all_df.isnull().sum() / len(all_df)) * 100\nall_df_na = all_df_na.drop(all_df_na[all_df_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_df_na})\nmissing_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_df_na.index, y=all_df_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.2 Imputing missing values","metadata":{}},{"cell_type":"markdown","source":"* PoolQC","metadata":{}},{"cell_type":"code","source":"# NA means \"No pool\" , so we can fill in with none\nall_df[\"PoolQC\"].unique()\nall_df[\"PoolQC\"] = all_df[\"PoolQC\"].fillna(\"None\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* MiscFeature","metadata":{}},{"cell_type":"code","source":"# NA means \"no misc feature\"\nall_df[\"MiscFeature\"].unique()\nall_df[\"MiscFeature\"] = all_df[\"MiscFeature\"].fillna(\"None\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Alley","metadata":{}},{"cell_type":"code","source":"all_df[\"Alley\"].unique()\nall_df[\"Alley\"] = all_df[\"Alley\"].fillna(\"None\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Fence","metadata":{}},{"cell_type":"code","source":"all_df[\"Fence\"] = all_df[\"Fence\"].fillna(\"None\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* FireplaceQu","metadata":{}},{"cell_type":"code","source":"all_df[\"FireplaceQu\"] = all_df[\"FireplaceQu\"].fillna(\"None\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* LotFrontage","metadata":{}},{"cell_type":"code","source":"#LotFrontage: Linear feet of street connected to property,Neighborhood: Physical locations within Ames city limits,fill in missing values by the median LotFrontage of the neighborhood\nall_df[\"LotFrontage\"] = all_df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* GarageType, GarageFinish, GarageQual and GarageCond","metadata":{}},{"cell_type":"code","source":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_df[col] = all_df[col].fillna('None')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* GarageYrBlt, GarageArea and GarageCars","metadata":{}},{"cell_type":"code","source":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_df[col] = all_df[col].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath(num)","metadata":{}},{"cell_type":"code","source":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_df[col] = all_df[col].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2(cat)","metadata":{}},{"cell_type":"code","source":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_df[col] = all_df[col].fillna('None')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* MasVnrArea and MasVnrType","metadata":{}},{"cell_type":"code","source":"all_df[\"MasVnrType\"] = all_df[\"MasVnrType\"].fillna(\"None\")\nall_df[\"MasVnrArea\"] = all_df[\"MasVnrArea\"].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* MSZoning (The general zoning classification)","metadata":{}},{"cell_type":"code","source":"all_df.MSZoning.value_counts()#The majority of the data are RL, so we can fill in the missing data with mode","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df['MSZoning'] = all_df['MSZoning'].fillna(all_df['MSZoning'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Utilities\n","metadata":{}},{"cell_type":"code","source":"all_df.Utilities.value_counts() #This feature won't help with the predicting so we can just delete it","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df = all_df.drop(['Utilities'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Functional","metadata":{}},{"cell_type":"code","source":"#data description says NA means typical\nall_df[\"Functional\"] = all_df[\"Functional\"].fillna(\"Typ\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Electrical","metadata":{}},{"cell_type":"code","source":"all_df.Electrical.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_df['Electrical'] = all_df['Electrical'].fillna(all_df['Electrical'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* KitchenQual","metadata":{}},{"cell_type":"code","source":"all_df['KitchenQual'] = all_df['KitchenQual'].fillna(all_df['KitchenQual'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Exterior1st and Exterior2nd","metadata":{}},{"cell_type":"code","source":"#only has one missing value so just fill in with mode\nall_df['Exterior1st'] = all_df['Exterior1st'].fillna(all_df['Exterior1st'].mode()[0])\nall_df['Exterior2nd'] = all_df['Exterior2nd'].fillna(all_df['Exterior2nd'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* SaleType","metadata":{}},{"cell_type":"code","source":"all_df['SaleType'] = all_df['SaleType'].fillna(all_df['SaleType'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* MSSubClass","metadata":{}},{"cell_type":"code","source":"all_df['MSSubClass'] = all_df['MSSubClass'].fillna(\"None\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#double check the missing values\nall_df_na = (all_df.isnull().sum() / len(all_df)) * 100\nall_df_na = all_df_na.drop(all_df_na[all_df_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_df_na})\nmissing_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.3 Data Correlation","metadata":{}},{"cell_type":"code","source":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train_df.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.4 Transform the datatype of numerical values that are really categorical","metadata":{}},{"cell_type":"code","source":"all_df['MSSubClass'] = all_df['MSSubClass'].apply(str)\nall_df['OverallCond'] = all_df['OverallCond'].astype(str)\n\nall_df['YrSold'] = all_df['YrSold'].astype(str)\nall_df['MoSold'] = all_df['MoSold'].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.5 Label encoding the categorical variables which have ordering set","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor f in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_df[f].values)) \n    all_df[f] = lbl.transform(list(all_df[f].values))\n        \nprint('Shape all_df: {}'.format(all_df.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.6 Creat new features","metadata":{}},{"cell_type":"code","source":"#Add total area\nall_df['TotalSF'] = all_df['TotalBsmtSF'] + all_df['1stFlrSF'] + all_df['2ndFlrSF']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Add skewed features\nnumeric_feats = all_df.dtypes[all_df.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.7 Box Cox Transformation of (highly) skewed features","metadata":{}},{"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_df[feat] = boxcox1p(all_df[feat], lam)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.8 Get dummy categorical features","metadata":{}},{"cell_type":"code","source":"all_df = pd.get_dummies(all_df)\nall_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.Modelling**","metadata":{}},{"cell_type":"code","source":"#Get new train and test data\nntrain = train_df.shape[0]\nntest = test_df.shape[0]\ntrain = all_df[:ntrain]\ntest = all_df[ntrain:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.1 Define cross validation strategy","metadata":{}},{"cell_type":"code","source":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Base models**","metadata":{}},{"cell_type":"markdown","source":"* LASSO Regression :","metadata":{}},{"cell_type":"code","source":"#make robust to outliers\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Elastic Net Regression :","metadata":{}},{"cell_type":"code","source":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Kernel Ridge Regression :","metadata":{}},{"cell_type":"code","source":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Gradient Boosting Regression :","metadata":{}},{"cell_type":"code","source":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* XGBoost :","metadata":{}},{"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* LightGBM :","metadata":{}},{"cell_type":"code","source":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nmodel_lgb2 = lgb.LGBMRegressor(objective='regression',num_leaves=15,\n                              learning_rate=0.5, n_estimators=70,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Stacking models**","metadata":{}},{"cell_type":"markdown","source":"Averaging base models:","metadata":{}},{"cell_type":"code","source":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #The predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WeightedModel():\n    def __init__(self, M1, M2):\n        self.M1 = M1\n        self.M2 = M2\n        \n    def fit(self,X,y):\n        for model in (self.M1,self.M2):\n            model.fit(X,y)\n        return self\n    \n    def predict(self,X):\n        prediction1 = self.M1.predict(X)\n        prediction2 = self.M2.predict(X)\n        return prediction1*0.3+prediction2*0.7","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Averaged base models score\n\naveraged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ensembling StackedRegressor, XGBoost and LightGBM**","metadata":{}},{"cell_type":"code","source":"#define a rmsle function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Final training and prediction**","metadata":{}},{"cell_type":"code","source":"#StackedRegressor:\nstacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGboost:\nmodel_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LightGBM:\nmodel_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}